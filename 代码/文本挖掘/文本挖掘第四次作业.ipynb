{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "614546c3",
   "metadata": {},
   "source": [
    "# 2110010114  李佳琪  数管211"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d28f2241",
   "metadata": {},
   "source": [
    "## 实验 1：Textrank：窗口全链接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8016e09d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "关键词及权重:\n",
      "线程: 1.0\n",
      "进程: 0.3570365106367063\n"
     ]
    }
   ],
   "source": [
    "import jieba.analyse\n",
    "\n",
    "def text_rank_keywords(text, num_keywords=2):\n",
    "    # Step 1: 分词和词性标注\n",
    "    words = list(jieba.posseg.cut(text))\n",
    "    \n",
    "    # Step 2: 去除停用词（长度小于2的单词）\n",
    "    filtered_words = [word.word for word in words if len(word.word) > 1]\n",
    "    \n",
    "    # Step 3: 使用TextRank计算各节点权重值\n",
    "    keywords = jieba.analyse.textrank(\" \".join(filtered_words), topK=num_keywords, withWeight=True)\n",
    "\n",
    "    return keywords\n",
    "\n",
    "# 示例文本\n",
    "sample_text = \"线程是程序执行时的最小单位，它是进程的一个执行流，\\\n",
    "是CPU调度和分派的基本单位，一个进程可以由很多个线程组成，\\\n",
    "线程间共享进程的所有资源，每个线程有自己的堆栈和局部变量。\\\n",
    "线程由CPU独立调度执行，在多CPU环境下就允许多个线程同时运行。\\\n",
    "同样多线程也可以实现并发操作，每个请求分配一个线程来处理\"\n",
    "\n",
    "# 提取关键词\n",
    "result = text_rank_keywords(sample_text)\n",
    "\n",
    "# 打印结果\n",
    "print(\"关键词及权重:\")\n",
    "for keyword, weight in result:\n",
    "    print(f\"{keyword}: {weight}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d4a7af8",
   "metadata": {},
   "source": [
    "## 实验课内容ii：配置Gensim!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "605c8e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF关键词:\n",
      "\n",
      "LSI关键词:\n",
      "\n",
      "LDA关键词:\n",
      "Topic 0: 0.5\n",
      "Topic 1: 0.5\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import TfidfModel, LsiModel, LdaModel\n",
    "from gensim import similarities\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# 示例文本\n",
    "documents = ['线程是程序执行时的最小单位，它是进程的一个执行流',\n",
    "             '是CPU调度和分派的基本单位，一个进程可以由很多个线程组成',\n",
    "             '线程间共享进程的所有资源，每个线程有自己的堆栈和局部变量',\n",
    "             '线程由CPU独立调度执行，在多CPU环境下就允许多个线程同时运行',\n",
    "             '同样多线程也可以实现并发操作，每个请求分配一个线程来处理']\n",
    "\n",
    "# 将文本处理为token列表\n",
    "texts = [simple_preprocess(doc) for doc in documents]\n",
    "\n",
    "# 构建词典\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# 构建文档-词频矩阵\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# TF-IDF模型\n",
    "tfidf_model = TfidfModel(corpus)\n",
    "tfidf_keywords = tfidf_model[dictionary.doc2bow(simple_preprocess(\"关键词提取\"))]\n",
    "\n",
    "# LSI模型\n",
    "lsi_model = LsiModel(corpus, id2word=dictionary)\n",
    "lsi_keywords = lsi_model[dictionary.doc2bow(simple_preprocess(\"关键词提取\"))]\n",
    "\n",
    "# LDA模型\n",
    "lda_model = LdaModel(corpus, id2word=dictionary, num_topics=2)\n",
    "lda_keywords = lda_model[dictionary.doc2bow(simple_preprocess(\"关键词提取\"))]\n",
    "\n",
    "# 打印结果\n",
    "print(\"TF-IDF关键词:\")\n",
    "for word_id, weight in tfidf_keywords:\n",
    "    print(f\"{dictionary[word_id]}: {weight}\")\n",
    "\n",
    "print(\"\\nLSI关键词:\")\n",
    "for topic_id, weight in lsi_keywords:\n",
    "    print(f\"Topic {topic_id}: {weight}\")\n",
    "\n",
    "print(\"\\nLDA关键词:\")\n",
    "for topic_id, weight in lda_keywords:\n",
    "    print(f\"Topic {topic_id}: {weight}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "86d9b769",
   "metadata": {},
   "source": [
    "## 实验课内容iii：gesim 的简单应用!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "347e9eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文档相似度:\n",
      "文档1: 0.0\n",
      "文档2: 0.0\n",
      "文档3: 0.0\n",
      "文档4: 0.0\n",
      "文档5: 0.0\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import TfidfModel\n",
    "from gensim import similarities\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# 示例文本\n",
    "documents = ['线程是程序执行时的最小单位，它是进程的一个执行流',\n",
    "             '是CPU调度和分派的基本单位，一个进程可以由很多个线程组成',\n",
    "             '线程间共享进程的所有资源，每个线程有自己的堆栈和局部变量',\n",
    "             '线程由CPU独立调度执行，在多CPU环境下就允许多个线程同时运行',\n",
    "             '同样多线程也可以实现并发操作，每个请求分配一个线程来处理']\n",
    "\n",
    "# 将文本处理为token列表\n",
    "texts = [simple_preprocess(doc) for doc in documents]\n",
    "\n",
    "# 构建词典\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# 构建文档-词频矩阵\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# 将二元组corpus转化为tf-idf向量\n",
    "tfidf_model = TfidfModel(corpus)\n",
    "tfidf_corpus = tfidf_model[corpus]\n",
    "\n",
    "# 存储训练好的tf-idf模型\n",
    "tfidf_model.save(\"tfidf_model\")\n",
    "\n",
    "# 计算文档相似度\n",
    "vec = tfidf_model[dictionary.doc2bow(simple_preprocess(\"关键词提取\"))]\n",
    "index = similarities.MatrixSimilarity(tfidf_corpus)\n",
    "\n",
    "# 计算文档vec与语料中每一篇文档的相似度\n",
    "sims = index[vec]\n",
    "\n",
    "# 打印结果\n",
    "print(\"文档相似度:\")\n",
    "for i, similarity in enumerate(sims):\n",
    "    print(f\"文档{i+1}: {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5ec5c3",
   "metadata": {},
   "source": [
    "## 实验课内容iiV：Textrank案例·实验课案例!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa0bbc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, similarities  \n",
    "import jieba  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcdbe47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss=['线程是程序执行时的最小单位，它是进程的一个执行流，\\是CPU调度和分派的基本单位，一个进程可以由很多个线程组成，\\线程间共享进程的所有资源，每个线程有自己的堆栈和局部变量。\\线程由CPU独立调度执行，在多CPU环境下就允许多个线程同时运行。\\同样多线程也可以实现并发操作，每个请求分配一个线程来处理。']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d0a6182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 3), (1, 4), (2, 3), (3, 3), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 2), (12, 2), (13, 1), (14, 1), (15, 2), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 2), (31, 1), (32, 1), (33, 3), (34, 1), (35, 1), (36, 1), (37, 2), (38, 1), (39, 1), (40, 1), (41, 2), (42, 5), (43, 1), (44, 7), (45, 1), (46, 1), (47, 1), (48, 2), (49, 1), (50, 1), (51, 3), (52, 1), (53, 7)]]\n"
     ]
    }
   ],
   "source": [
    "# 使用jieba进行分词  \n",
    "ss_seg = [list(jieba.cut(s)) for s in ss]  \n",
    "\n",
    "# 创建字典对象  \n",
    "dictionary = corpora.Dictionary(ss_seg)  \n",
    "  \n",
    "# 将字典对象转化为corpus对象  \n",
    "corpus = [dictionary.doc2bow(s) for s in ss_seg]  \n",
    "print (corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49956a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic for document 1:\n",
      "{'CPU': 0, '\\\\': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda\\exe\\Lib\\site-packages\\gensim\\models\\lsimodel.py:102: RuntimeWarning: invalid value encountered in divide\n",
      "  rel_spectrum = np.abs(1.0 - np.cumsum(s / np.sum(s)))\n",
      "F:\\anaconda\\exe\\Lib\\site-packages\\gensim\\models\\ldamodel.py:847: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  perwordbound = self.bound(chunk, subsample_ratio=subsample_ratio) / (subsample_ratio * corpus_words)\n"
     ]
    }
   ],
   "source": [
    "# 使用TF-IDF模型  \n",
    "tfidf = models.TfidfModel(corpus)  \n",
    "corpus_tfidf = tfidf[corpus]  \n",
    "  \n",
    "# 使用LSI模型  \n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)  \n",
    "corpus_lsi = lsi[corpus_tfidf]  \n",
    "  \n",
    "# 使用LDA模型  \n",
    "lda = models.LdaModel(corpus_lsi, id2word=dictionary, num_topics=2, random_state=42)  \n",
    "corpus_lda = lda[corpus_lsi]  \n",
    "  \n",
    "# 打印关键词及其权重  \n",
    "for i in range(len(corpus)):  \n",
    "    print(f\"Topic for document {i+1}:\")  \n",
    "    print(dict(zip(dictionary.values(), [row[i] for row in corpus_lda[i]])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "673affa4",
   "metadata": {},
   "source": [
    "## 实验课内容V：封装函数!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99894629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF模型结果：\n",
      "晋江市/ 城市/ 大会/ 爱心/ 中华/ 基金会/ 陈健倩/ 重庆市/ 许嘉璐/ 巡视员/ \n",
      "TextRank模型结果：\n",
      "城市/ 爱心/ 救助/ 中国/ 社会/ 晋江市/ 基金会/ 大会/ 介绍/ 公益活动/ \n",
      "LSI模型结果：\n",
      "中国/ 中华/ 爱心/ 项目/ 社会/ 基金会/ 城市/ 公益活动/ 全国/ 国家/ \n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import math\n",
    "import jieba\n",
    "import jieba.posseg as psg\n",
    "from gensim import corpora, models\n",
    "from jieba import analyse\n",
    "import functools\n",
    "\n",
    "\n",
    "# 停用词表加载方法\n",
    "def get_stopword_list():\n",
    "    # 停用词表存储路径，每一行为一个词，按行读取进行加载\n",
    "    # 进行编码转换确保匹配准确率\n",
    "    stop_word_path = 'D:\\\\大三上\\\\文本挖掘\\\\作业4\\\\stopword.txt'\n",
    "    stopword_list = [sw.replace('\\n', '') for sw in open(stop_word_path,encoding='utf-8').readlines()]\n",
    "    return stopword_list\n",
    "\n",
    "\n",
    "# 分词方法，调用结巴接口\n",
    "def seg_to_list(sentence, pos=False):\n",
    "    if not pos:\n",
    "        # 不进行词性标注的分词方法\n",
    "        seg_list = jieba.cut(sentence)\n",
    "    else:\n",
    "        # 进行词性标注的分词方法\n",
    "        seg_list = psg.cut(sentence)\n",
    "    return seg_list\n",
    "\n",
    "\n",
    "# idf值统计方法\n",
    "def train_idf(doc_list):\n",
    "    idf_dic = {}\n",
    "    # 总文档数\n",
    "    tt_count = len(doc_list)\n",
    "\n",
    "    # 每个词出现的文档数\n",
    "    for doc in doc_list:\n",
    "        for word in set(doc):\n",
    "            idf_dic[word] = idf_dic.get(word, 0.0) + 1.0\n",
    "\n",
    "    # 按公式转换为idf值，分母加1进行平滑处理\n",
    "    for k, v in idf_dic.items():\n",
    "        idf_dic[k] = math.log(tt_count / (1.0 + v))\n",
    "\n",
    "    # 对于没有在字典中的词，默认其仅在一个文档出现，得到默认idf值\n",
    "    default_idf = math.log(tt_count / (1.0))\n",
    "    return idf_dic, default_idf\n",
    "\n",
    "\n",
    "#  排序函数，用于topK关键词的按值排序\n",
    "def cmp(e1, e2):\n",
    "    import numpy as np\n",
    "    res = np.sign(e1[1] - e2[1])\n",
    "    if res != 0:\n",
    "        return res\n",
    "    else:\n",
    "        a = e1[0] + e2[0]\n",
    "        b = e2[0] + e1[0]\n",
    "        if a > b:\n",
    "            return 1\n",
    "        elif a == b:\n",
    "            return 0\n",
    "        else:\n",
    "            return -1\n",
    "\n",
    "# TF-IDF类\n",
    "class TfIdf(object):\n",
    "    # 四个参数分别是：训练好的idf字典，默认idf值，处理后的待提取文本，关键词数量\n",
    "    def __init__(self, idf_dic, default_idf, word_list, keyword_num):\n",
    "        self.word_list = word_list\n",
    "        self.idf_dic, self.default_idf = idf_dic, default_idf\n",
    "        self.tf_dic = self.get_tf_dic()\n",
    "        self.keyword_num = keyword_num\n",
    "\n",
    "    # 统计tf值\n",
    "    def get_tf_dic(self):\n",
    "        tf_dic = {}\n",
    "        for word in self.word_list:\n",
    "            tf_dic[word] = tf_dic.get(word, 0.0) + 1.0\n",
    "\n",
    "        tt_count = len(self.word_list)\n",
    "        for k, v in tf_dic.items():\n",
    "            tf_dic[k] = float(v) / tt_count\n",
    "\n",
    "        return tf_dic\n",
    "\n",
    "    # 按公式计算tf-idf\n",
    "    def get_tfidf(self):\n",
    "        tfidf_dic = {}\n",
    "        for word in self.word_list:\n",
    "            idf = self.idf_dic.get(word, self.default_idf)\n",
    "            tf = self.tf_dic.get(word, 0)\n",
    "\n",
    "            tfidf = tf * idf\n",
    "            tfidf_dic[word] = tfidf\n",
    "\n",
    "        tfidf_dic.items()\n",
    "        # 根据tf-idf排序，去排名前keyword_num的词作为关键词\n",
    "        for k, v in sorted(tfidf_dic.items(), key=functools.cmp_to_key(cmp), reverse=True)[:self.keyword_num]:\n",
    "            print(k + \"/ \", end='')\n",
    "        print()\n",
    "\n",
    "\n",
    "# 主题模型\n",
    "class TopicModel(object):\n",
    "    # 三个传入参数：处理后的数据集，关键词数量，具体模型（LSI、LDA），主题数量\n",
    "    def __init__(self, doc_list, keyword_num, model='LSI', num_topics=4):\n",
    "        # 使用gensim的接口，将文本转为向量化表示\n",
    "        # 先构建词空间\n",
    "        self.dictionary = corpora.Dictionary(doc_list)\n",
    "        # 使用BOW模型向量化\n",
    "        corpus = [self.dictionary.doc2bow(doc) for doc in doc_list]\n",
    "        # 对每个词，根据tf-idf进行加权，得到加权后的向量表示\n",
    "        self.tfidf_model = models.TfidfModel(corpus)\n",
    "        self.corpus_tfidf = self.tfidf_model[corpus]\n",
    "\n",
    "        self.keyword_num = keyword_num\n",
    "        self.num_topics = num_topics\n",
    "        # 选择加载的模型\n",
    "        self.model = self.train_lsi()\n",
    "\n",
    "        # 得到数据集的主题-词分布\n",
    "        word_dic = self.word_dictionary(doc_list)\n",
    "        self.wordtopic_dic = self.get_wordtopic(word_dic)\n",
    "\n",
    "    def train_lsi(self):\n",
    "        lsi = models.LsiModel(self.corpus_tfidf, id2word=self.dictionary, num_topics=self.num_topics)\n",
    "        return lsi\n",
    "\n",
    "    def train_lda(self):\n",
    "        lda = models.LdaModel(self.corpus_tfidf, id2word=self.dictionary, num_topics=self.num_topics)\n",
    "        return lda\n",
    "\n",
    "    def get_wordtopic(self, word_dic):\n",
    "        wordtopic_dic = {}\n",
    "\n",
    "        for word in word_dic:\n",
    "            single_list = [word]\n",
    "            wordcorpus = self.tfidf_model[self.dictionary.doc2bow(single_list)]\n",
    "            wordtopic = self.model[wordcorpus]\n",
    "            wordtopic_dic[word] = wordtopic\n",
    "        return wordtopic_dic\n",
    "\n",
    "    # 计算词的分布和文档的分布的相似度，取相似度最高的keyword_num个词作为关键词\n",
    "    def get_simword(self, word_list):\n",
    "        sentcorpus = self.tfidf_model[self.dictionary.doc2bow(word_list)]\n",
    "        senttopic = self.model[sentcorpus]\n",
    "\n",
    "        # 余弦相似度计算\n",
    "        def calsim(l1, l2):\n",
    "            a, b, c = 0.0, 0.0, 0.0\n",
    "            for t1, t2 in zip(l1, l2):\n",
    "                x1 = t1[1]\n",
    "                x2 = t2[1]\n",
    "                a += x1 * x1\n",
    "                b += x1 * x1\n",
    "                c += x2 * x2\n",
    "            sim = a / math.sqrt(b * c) if not (b * c) == 0.0 else 0.0\n",
    "            return sim\n",
    "\n",
    "        # 计算输入文本和每个词的主题分布相似度\n",
    "        sim_dic = {}\n",
    "        for k, v in self.wordtopic_dic.items():\n",
    "            if k not in word_list:\n",
    "                continue\n",
    "            sim = calsim(v, senttopic)\n",
    "            sim_dic[k] = sim\n",
    "\n",
    "        for k, v in sorted(sim_dic.items(), key=functools.cmp_to_key(cmp), reverse=True)[:self.keyword_num]:\n",
    "            print(k + \"/ \", end='')\n",
    "        print()\n",
    "\n",
    "    # 词空间构建方法和向量化方法，在没有gensim接口时的一般处理方法\n",
    "    def word_dictionary(self, doc_list):\n",
    "        dictionary = []\n",
    "        for doc in doc_list:\n",
    "            dictionary.extend(doc)\n",
    "\n",
    "        dictionary = list(set(dictionary))\n",
    "\n",
    "        return dictionary\n",
    "\n",
    "    def doc2bowvec(self, word_list):\n",
    "        vec_list = [1 if word in word_list else 0 for word in self.dictionary]\n",
    "        return vec_list\n",
    "\n",
    "\n",
    "def tfidf_extract(word_list, pos=False, keyword_num=10):\n",
    "    doc_list = load_data(pos)\n",
    "    idf_dic, default_idf = train_idf(doc_list)\n",
    "    tfidf_model = TfIdf(idf_dic, default_idf, word_list, keyword_num)\n",
    "    tfidf_model.get_tfidf()\n",
    "\n",
    "\n",
    "def textrank_extract(text, pos=False, keyword_num=10):\n",
    "    textrank = analyse.textrank\n",
    "    keywords = textrank(text, keyword_num)\n",
    "    # 输出抽取出的关键词\n",
    "    for keyword in keywords:\n",
    "        print(keyword + \"/ \", end='')\n",
    "    print()\n",
    "\n",
    "\n",
    "def topic_extract(word_list, model, pos=False, keyword_num=10):\n",
    "    doc_list = load_data(pos)\n",
    "    topic_model = TopicModel(doc_list, keyword_num, model=model)\n",
    "    topic_model.get_simword(word_list)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    text = '6月19日,《2012年度“中国爱心城市”公益活动新闻发布会》在京举行。' + \\\n",
    "           '中华社会救助基金会理事长许嘉璐到会讲话。基金会高级顾问朱发忠,全国老龄' + \\\n",
    "           '办副主任朱勇,民政部社会救助司助理巡视员周萍,中华社会救助基金会副理事长耿志远,' + \\\n",
    "           '重庆市民政局巡视员谭明政。晋江市人大常委会主任陈健倩,以及10余个省、市、自治区民政局' + \\\n",
    "           '领导及四十多家媒体参加了发布会。中华社会救助基金会秘书长时正新介绍本年度“中国爱心城' + \\\n",
    "           '市”公益活动将以“爱心城市宣传、孤老关爱救助项目及第二届中国爱心城市大会”为主要内容,重庆市' + \\\n",
    "           '、呼和浩特市、长沙市、太原市、蚌埠市、南昌市、汕头市、沧州市、晋江市及遵化市将会积极参加' + \\\n",
    "           '这一公益活动。中国雅虎副总编张银生和凤凰网城市频道总监赵耀分别以各自媒体优势介绍了活动' + \\\n",
    "           '的宣传方案。会上,中华社会救助基金会与“第二届中国爱心城市大会”承办方晋江市签约,许嘉璐理' + \\\n",
    "           '事长接受晋江市参与“百万孤老关爱行动”向国家重点扶贫地区捐赠的价值400万元的款物。晋江市人大' + \\\n",
    "           '常委会主任陈健倩介绍了大会的筹备情况。'\n",
    "\n",
    "    pos = True\n",
    "    seg_list = seg_to_list(text, pos)\n",
    "    filter_list = word_filter(seg_list, pos)\n",
    "\n",
    "    print('TF-IDF模型结果：')\n",
    "    tfidf_extract(filter_list)\n",
    "    print('TextRank模型结果：')\n",
    "    textrank_extract(text)\n",
    "    print('LSI模型结果：')\n",
    "    topic_extract(filter_list, 'LSI', pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a693a2ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
